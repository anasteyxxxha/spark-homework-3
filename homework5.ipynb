{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b509bd91-2da2-406a-b6fe-e02d58b462ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9c1dd58-e653-458a-8602-e242362349c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/11 08:16:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdfd6164-ccde-4e51-8058-c3ba89528691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-11 08:15:54,838 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `/tmp/crime.csv': File exists\n",
      "2025-01-11 08:15:55,828 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "put: `/tmp/offense_codes.csv': File exists\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put crime.csv /tmp/\n",
    "!hdfs dfs -put offense_codes.csv /tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f9fd298-4f56-4347-8147-9c7308da3934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crimeFacts = (spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"/tmp/crime.csv\")\n",
    "    .cache())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5064ea5e-b091-4a81-b205-5b77fe8158ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------------+-----------+------------+--------------------+\n",
      "|INCIDENT_NUMBER|OFFENSE_CODE|  OFFENSE_CODE_GROUP| OFFENSE_DESCRIPTION|DISTRICT|REPORTING_AREA|SHOOTING|   OCCURRED_ON_DATE|YEAR|MONTH|DAY_OF_WEEK|HOUR|  UCR_PART|           STREET|        Lat|        Long|            Location|\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------------+-----------+------------+--------------------+\n",
      "|     I182070945|         619|             Larceny|  LARCENY ALL OTHERS|     D14|           808|    null|2018-09-02 13:00:00|2018|    9|     Sunday|  13|  Part One|       LINCOLN ST|42.35779134|-71.13937053|(42.35779134, -71...|\n",
      "|     I182070943|        1402|           Vandalism|           VANDALISM|     C11|           347|    null|2018-08-21 00:00:00|2018|    8|    Tuesday|   0|  Part Two|         HECLA ST|42.30682138|-71.06030035|(42.30682138, -71...|\n",
      "|     I182070941|        3410|               Towed| TOWED MOTOR VEHICLE|      D4|           151|    null|2018-09-03 19:27:00|2018|    9|     Monday|  19|Part Three|      CAZENOVE ST|42.34658879|-71.07242943|(42.34658879, -71...|\n",
      "|     I182070940|        3114|Investigate Property|INVESTIGATE PROPERTY|      D4|           272|    null|2018-09-03 21:16:00|2018|    9|     Monday|  21|Part Three|       NEWCOMB ST|42.33418175|-71.07866441|(42.33418175, -71...|\n",
      "|     I182070938|        3114|Investigate Property|INVESTIGATE PROPERTY|      B3|           421|    null|2018-09-03 21:05:00|2018|    9|     Monday|  21|Part Three|         DELHI ST|42.27536542|-71.09036101|(42.27536542, -71...|\n",
      "|     I182070936|        3820|Motor Vehicle Acc...|M/V ACCIDENT INVO...|     C11|           398|    null|2018-09-03 21:09:00|2018|    9|     Monday|  21|Part Three|       TALBOT AVE|42.29019621|-71.07159012|(42.29019621, -71...|\n",
      "|     I182070933|         724|          Auto Theft|          AUTO THEFT|      B2|           330|    null|2018-09-03 21:25:00|2018|    9|     Monday|  21|  Part One|      NORMANDY ST|42.30607218| -71.0827326|(42.30607218, -71...|\n",
      "|     I182070932|        3301|     Verbal Disputes|      VERBAL DISPUTE|      B2|           584|    null|2018-09-03 20:39:37|2018|    9|     Monday|  20|Part Three|          LAWN ST|42.32701648|-71.10555088|(42.32701648, -71...|\n",
      "|     I182070931|         301|             Robbery|    ROBBERY - STREET|      C6|           177|    null|2018-09-03 20:48:00|2018|    9|     Monday|  20|  Part One|MASSACHUSETTS AVE|42.33152148|-71.07085307|(42.33152148, -71...|\n",
      "|     I182070929|        3301|     Verbal Disputes|      VERBAL DISPUTE|     C11|           364|    null|2018-09-03 20:38:00|2018|    9|     Monday|  20|Part Three|        LESLIE ST|42.29514664|-71.05860832|(42.29514664, -71...|\n",
      "|     I182070928|        3301|     Verbal Disputes|      VERBAL DISPUTE|      C6|           913|    null|2018-09-03 19:55:00|2018|    9|     Monday|  19|Part Three|    OCEAN VIEW DR|42.31957856|-71.04032766|(42.31957856, -71...|\n",
      "|     I182070927|        3114|Investigate Property|INVESTIGATE PROPERTY|      C6|           936|    null|2018-09-03 20:19:00|2018|    9|     Monday|  20|Part Three|      DALESSIO CT|42.34011469|-71.05339029|(42.34011469, -71...|\n",
      "|     I182070923|        3108|Fire Related Reports|FIRE REPORT - HOU...|      D4|           139|    null|2018-09-03 19:58:00|2018|    9|     Monday|  19|Part Three|   MARLBOROUGH ST| 42.3503876| -71.0878529|(42.35038760, -71...|\n",
      "|     I182070922|        2647|               Other|THREATS TO DO BOD...|      B3|           429|    null|2018-09-03 20:39:00|2018|    9|     Monday|  20|  Part Two|      WOODROW AVE|42.28647012|-71.08714661|(42.28647012, -71...|\n",
      "|     I182070921|        3201|       Property Lost|     PROPERTY - LOST|      B3|           469|    null|2018-09-02 14:00:00|2018|    9|     Sunday|  14|Part Three|        MULVEY ST|42.27924052|-71.09667382|(42.27924052, -71...|\n",
      "|     I182070920|        3006|  Medical Assistance|SICK/INJURED/MEDI...|    null|              |    null|2018-09-03 19:43:00|2018|    9|     Monday|  19|Part Three|             null|42.35287456| -71.0738297|(42.35287456, -71...|\n",
      "|     I182070919|        3301|     Verbal Disputes|      VERBAL DISPUTE|     C11|           341|    null|2018-09-03 18:52:00|2018|    9|     Monday|  18|Part Three|    STONEHURST ST|42.30526428|-71.06683755|(42.30526428, -71...|\n",
      "|     I182070918|        3305|Assembly or Gathe...| DEMONSTRATIONS/RIOT|      D4|           130|    null|2018-09-03 17:00:00|2018|    9|     Monday|  17|Part Three|   HUNTINGTON AVE|42.34857652|-71.07772012|(42.34857652, -71...|\n",
      "|     I182070917|        2647|               Other|THREATS TO DO BOD...|      B2|           901|    null|2018-09-03 19:52:00|2018|    9|     Monday|  19|  Part Two|      HORADAN WAY|42.33371742|-71.09665806|(42.33371742, -71...|\n",
      "|     I182070915|         614|Larceny From Moto...|LARCENY THEFT FRO...|      B2|           181|    null|2018-09-02 18:00:00|2018|    9|     Sunday|  18|  Part One|       SHIRLEY ST| 42.3256949|-71.06816778|(42.32569490, -71...|\n",
      "+---------------+------------+--------------------+--------------------+--------+--------------+--------+-------------------+----+-----+-----------+----+----------+-----------------+-----------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crimeFacts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a445db4-9089-42e5-bf20-d9bf39acddcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "offenseCodes = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"/tmp/offense_codes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2080993b-25fd-4e07-81f2-819258b7da58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|CODE|                NAME|\n",
      "+----+--------------------+\n",
      "| 612|LARCENY PURSE SNA...|\n",
      "| 613| LARCENY SHOPLIFTING|\n",
      "| 615|LARCENY THEFT OF ...|\n",
      "|1731|              INCEST|\n",
      "|3111|LICENSE PREMISE V...|\n",
      "|2646|LIQUOR - DRINKING...|\n",
      "|2204|LIQUOR LAW VIOLATION|\n",
      "|3810|M/V ACCIDENT - IN...|\n",
      "|3801|M/V ACCIDENT - OTHER|\n",
      "|3807|M/V ACCIDENT - OT...|\n",
      "|3803|M/V ACCIDENT - PE...|\n",
      "|3805|M/V ACCIDENT - PO...|\n",
      "|3802|M/V ACCIDENT - PR...|\n",
      "|3205|   M/V PLATES - LOST|\n",
      "| 123|MANSLAUGHTER - NO...|\n",
      "| 121|MANSLAUGHTER - VE...|\n",
      "|3501|      MISSING PERSON|\n",
      "|3502|MISSING PERSON - ...|\n",
      "|3503|MISSING PERSON - ...|\n",
      "| 111|MURDER, NON-NEGLI...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offenseCodes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9aabbdb-d488-4563-8002-2048a84c6af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, count, mean, month, median, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f58b8a-1010-4e53-83e3-9aea2a66d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeFacts = crimeFacts.withColumn(\"OCCURRED_ON_DATE\", crimeFacts[\"OCCURRED_ON_DATE\"].cast(\"timestamp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26001f4e-62b5-48b1-89c2-6e3ffb656aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeFacts = (\n",
    "    crimeFacts.withColumn(\"year\", year(col(\"OCCURRED_ON_DATE\")))\n",
    "    .withColumn(\"month\", month(col(\"OCCURRED_ON_DATE\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5b0b007-de6f-497c-bebe-e3cf9e202a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|DISTRICT|crimes_total|\n",
      "+--------+------------+\n",
      "|      C6|       23460|\n",
      "|    null|        1765|\n",
      "|      B2|       49945|\n",
      "|     C11|       42530|\n",
      "|     E13|       17536|\n",
      "|      B3|       35442|\n",
      "|      E5|       13239|\n",
      "|     A15|        6505|\n",
      "|      A7|       13544|\n",
      "|     D14|       20127|\n",
      "|      D4|       41915|\n",
      "|     E18|       17348|\n",
      "|      A1|       35717|\n",
      "+--------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "crimes_total = crimeFacts.groupBy(\"DISTRICT\").count().withColumnRenamed(\"count\", \"crimes_total\")\n",
    "crimes_total.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bc1f72d-ff0e-45bf-8fc4-b9a60f24ced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------+\n",
      "|DISTRICT|crimes_monthly|\n",
      "+--------+--------------+\n",
      "|      C6|        1870.0|\n",
      "|    null|         109.5|\n",
      "|      B2|        3986.0|\n",
      "|     C11|        3284.0|\n",
      "|     E13|        1368.5|\n",
      "|      B3|        2764.0|\n",
      "|      E5|        1043.5|\n",
      "|     A15|         499.0|\n",
      "|      A7|        1092.0|\n",
      "|     D14|        1607.5|\n",
      "|      D4|        3297.5|\n",
      "|     E18|        1313.0|\n",
      "|      A1|        2775.0|\n",
      "+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monthly_crimes = (\n",
    "    crimeFacts.withColumn(\"month\", month(col(\"OCCURRED_ON_DATE\")))\n",
    "    .groupBy(\"DISTRICT\", \"month\")\n",
    "    .count()\n",
    "    .groupBy(\"DISTRICT\")\n",
    "    .agg(median(col(\"count\"))\n",
    "    .alias(\"crimes_monthly\"))\n",
    ")\n",
    "monthly_crimes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa463131-9212-4947-a67b-fcde43f89ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------------+\n",
      "|DISTRICT|          avg(Lat)|         avg(Long)|\n",
      "+--------+------------------+------------------+\n",
      "|      C6| 42.21212258445509|-70.85561011772268|\n",
      "|    null| 25.23950519369344|-43.44877438704257|\n",
      "|      B2| 42.31600367732632| -71.0756993065438|\n",
      "|     C11| 42.29263740899965|-71.05125995734306|\n",
      "|     E13|42.309803655709324|-71.09800478878299|\n",
      "|      B3| 42.28305944520085|-71.07894914185519|\n",
      "|      E5|42.197969994470235|-71.00440862434776|\n",
      "|     A15|42.179155250910874|-70.74472508958492|\n",
      "|      A7| 42.36070260499406| -71.0039483303988|\n",
      "|     D14|  42.3435072451093|-71.13125461726422|\n",
      "|      D4|42.341242517909265|-71.07725024947149|\n",
      "|     E18|  42.2626806112253| -71.1189199875768|\n",
      "|      A1| 42.33123077259858|-71.01991881361955|\n",
      "+--------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coordinates = crimeFacts.groupBy(\"DISTRICT\").agg(mean(col(\"Lat\")), mean(col(\"Long\")))\n",
    "coordinates.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb472760-1a92-41df-bb45-c0caa4f0a64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------+------------------+------------------+\n",
      "|District|crimes_total|crimes_monthly|lat               |lng               |\n",
      "+--------+------------+--------------+------------------+------------------+\n",
      "|null    |1765        |null          |null              |null              |\n",
      "|A1      |35717       |2775.0        |42.33123077259858 |-71.01991881361955|\n",
      "|A15     |6505        |499.0         |42.179155250910874|-70.74472508958492|\n",
      "|A7      |13544       |1092.0        |42.36070260499406 |-71.0039483303988 |\n",
      "|B2      |49945       |3986.0        |42.31600367732632 |-71.0756993065438 |\n",
      "|B3      |35442       |2764.0        |42.28305944520085 |-71.07894914185519|\n",
      "|C11     |42530       |3284.0        |42.29263740899965 |-71.05125995734306|\n",
      "|C6      |23460       |1870.0        |42.21212258445509 |-70.85561011772268|\n",
      "|D14     |20127       |1607.5        |42.3435072451093  |-71.13125461726422|\n",
      "|D4      |41915       |3297.5        |42.341242517909265|-71.07725024947149|\n",
      "|E13     |17536       |1368.5        |42.309803655709324|-71.09800478878299|\n",
      "|E18     |17348       |1313.0        |42.2626806112253  |-71.1189199875768 |\n",
      "|E5      |13239       |1043.5        |42.197969994470235|-71.00440862434776|\n",
      "+--------+------------+--------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "district_metrics = (\n",
    "    crimes_total.join(monthly_crimes, on=\"District\", how=\"left_outer\")\n",
    "    .join(coordinates, on=\"District\", how=\"left_outer\")\n",
    "    .select(\n",
    "        \"District\",\n",
    "        \"crimes_total\",\n",
    "        \"crimes_monthly\",\n",
    "        col(\"avg(Lat)\").alias(\"lat\"),\n",
    "        col(\"avg(Long)\").alias(\"lng\"))\n",
    ")\n",
    "\n",
    "district_metrics.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b40336e8-636d-44a2-9231-7a4051c807dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pyarrow in /home/jovyan/.local/lib/python3.11/site-packages (18.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/jovyan/.local/lib/python3.11/site-packages (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install numpy\n",
    "import numpy as np\n",
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fd63f7f-662f-4264-82da-1473f405d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/11 08:18:25 WARN TaskSetManager: Lost task 0.0 in stage 36.0 (TID 39) (worker1 executor 1): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 814, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/01/11 08:18:25 WARN TaskSetManager: Lost task 0.1 in stage 36.0 (TID 40) (worker2 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 814, in main\n",
      "    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n",
      "                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n",
      "    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n",
      "    f, return_type = read_command(pickleSer, infile)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n",
      "    command = serializer._read_with_length(file)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n",
      "    return self.loads(obj)\n",
      "           ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n",
      "    return cloudpickle.loads(obj, encoding=encoding)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000003/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n",
      "    __import__(name)\n",
      "ModuleNotFoundError: No module named 'numpy'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:118)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:891)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:891)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/01/11 08:18:25 ERROR TaskSetManager: Task 0 in stage 36.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'numpy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 11\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mcumsum(x))\n\u001b[1;32m      5\u001b[0m cumsum_all_periods \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m     crimeFacts\u001b[38;5;241m.\u001b[39mgroupBy(F\u001b[38;5;241m.\u001b[39mmonth(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOCCURRED_ON_DATE\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      7\u001b[0m       \u001b[38;5;241m.\u001b[39magg(F\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m       \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_count\u001b[39m\u001b[38;5;124m'\u001b[39m, cumulative_sum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 11\u001b[0m \u001b[43mcumsum_all_periods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:901\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    896\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    897\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    898\u001b[0m     )\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 901\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 814, in main\n    func, profiler, deserializer, serializer = read_udfs(pickleSer, infile, eval_type)\n                                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 650, in read_udfs\n    udfs.append(read_single_udf(pickleSer, infile, eval_type, runner_conf, udf_index=i))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 375, in read_single_udf\n    f, return_type = read_command(pickleSer, infile)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/worker.py\", line 87, in read_command\n    command = serializer._read_with_length(file)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 174, in _read_with_length\n    return self.loads(obj)\n           ^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/serializers.py\", line 472, in loads\n    return cloudpickle.loads(obj, encoding=encoding)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/hadoop-jupyter/nm-local-dir/usercache/jupyter/appcache/application_1736583312854_0001/container_1736583312854_0001_01_000002/pyspark.zip/pyspark/cloudpickle/cloudpickle.py\", line 649, in subimport\n    __import__(name)\nModuleNotFoundError: No module named 'numpy'\n"
     ]
    }
   ],
   "source": [
    "@F.pandas_udf('int')\n",
    "def cumulative_sum(s):\n",
    "    return s.apply(lambda x: np.cumsum(x))\n",
    "\n",
    "cumsum_all_periods = (\n",
    "    crimeFacts.groupBy(F.month('OCCURRED_ON_DATE'))\n",
    "      .agg(F.count('*').alias('count'))\n",
    "      .withColumn('cumulative_count', cumulative_sum('count'))\n",
    ")\n",
    "\n",
    "cumsum_all_periods.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "903ba610-0dbd-4045-b7ea-f38c2fb177bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid function: pandas_udf with function type GROUPED_MAP or the function in groupby.applyInPandas must take either one argument (data) or two arguments (key, data).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m cumsum_by_year \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      2\u001b[0m     \u001b[43mcrimeFacts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43myear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOCCURRED_ON_DATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmonth\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOCCURRED_ON_DATE\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m*\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43myear(OCCURRED_ON_DATE)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapplyInPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcumulative_sum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmonth int, count int, cumulative_count int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/pandas/group_ops.py:224\u001b[0m, in \u001b[0;36mPandasGroupedOpsMixin.applyInPandas\u001b[0;34m(self, func, schema)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandas_udf, PandasUDFType\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, GroupedData)\n\u001b[0;32m--> 224\u001b[0m udf \u001b[38;5;241m=\u001b[39m \u001b[43mpandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctionType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPandasUDFType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGROUPED_MAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df\n\u001b[1;32m    226\u001b[0m udf_column \u001b[38;5;241m=\u001b[39m udf(\u001b[38;5;241m*\u001b[39m[df[col] \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns])\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/pandas/functions.py:387\u001b[0m, in \u001b[0;36mpandas_udf\u001b[0;34m(f, returnType, functionType)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(_create_pandas_udf, returnType\u001b[38;5;241m=\u001b[39mreturn_type, evalType\u001b[38;5;241m=\u001b[39meval_type)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_pandas_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/pandas/functions.py:443\u001b[0m, in \u001b[0;36m_create_pandas_udf\u001b[0;34m(f, returnType, evalType)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid function: 0-arg pandas_udfs are not supported. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead, create a 1-arg pandas_udf and ignore the arg in your function.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    440\u001b[0m     )\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_GROUPED_MAP_PANDAS_UDF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(argspec\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid function: pandas_udf with function type GROUPED_MAP or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe function in groupby.applyInPandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust take either one argument (data) or two arguments (key, data).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m     )\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m evalType \u001b[38;5;241m==\u001b[39m PythonEvalType\u001b[38;5;241m.\u001b[39mSQL_COGROUPED_MAP_PANDAS_UDF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(argspec\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m    450\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid function: the function in cogroup.applyInPandas \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmust take either two arguments (left, right) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor three arguments (key, left, right).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    454\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid function: pandas_udf with function type GROUPED_MAP or the function in groupby.applyInPandas must take either one argument (data) or two arguments (key, data)."
     ]
    }
   ],
   "source": [
    "cumsum_by_year = (\n",
    "    crimeFacts.groupBy(F.year('OCCURRED_ON_DATE'), F.month('OCCURRED_ON_DATE'))\n",
    "      .agg(F.count('*').alias('count'))\n",
    "      .groupby('year(OCCURRED_ON_DATE)')\n",
    "      .applyInPandas(cumulative_sum, schema='month int, count int, cumulative_count int')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae401f47-3b03-416c-8213-5f22a4eb4be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
